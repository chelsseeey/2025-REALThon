import sys
import os
import json
import argparse
from pathlib import Path
from decimal import Decimal

sys.stdout.reconfigure(encoding='utf-8')

# ==========================================
# 1. ë°±ì—”ë“œ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ ì„¤ì •
# ==========================================
# analysis_wrapper.pyê°€ backend í´ë”ì— ìˆìœ¼ë¯€ë¡œ í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ ì‚¬ìš©
CURRENT_DIR = Path(__file__).parent.absolute()
BACKEND_DIR = CURRENT_DIR

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(BACKEND_DIR))

# dotenvë¥¼ ë¨¼ì € import (ë‹¤ë¥¸ ëª¨ë“ˆë“¤ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡)
from dotenv import load_dotenv
load_dotenv()

# ë°±ì—”ë“œ ëª¨ë“ˆ import
try:
    from test_parse import parse_exam
    from score import parse_sheet
    from parse2 import parse_student_answer_handwriting
    import clustering
    from clustering import (
        load_exams, get_embeddings, cosine_similarity_matrix, 
        cluster_by_threshold, compute_cluster_stats, describe_clusters_with_openai
    )
    # clustering ëª¨ë“ˆì—ì„œ ìƒìˆ˜ ê°€ì ¸ì˜¤ê¸°
    SIM_THRESHOLD = getattr(clustering, 'SIM_THRESHOLD', 0.90)
    EMBED_MODEL = getattr(clustering, 'EMBED_MODEL', 'text-embedding-3-large')
    CLUSTER_SUMMARY_MODEL = getattr(clustering, 'CLUSTER_SUMMARY_MODEL', 'gpt-4o-mini')
    MAX_SAMPLES_PER_CLUSTER = getattr(clustering, 'MAX_SAMPLES_PER_CLUSTER', 10)
    CLUSTERING_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ì¼ë¶€ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}", file=sys.stderr)
    import traceback
    traceback.print_exc()
    CLUSTERING_AVAILABLE = False
    # ê¸°ë³¸ê°’ ì„¤ì •
    SIM_THRESHOLD = 0.90
    EMBED_MODEL = "text-embedding-3-large"
    CLUSTER_SUMMARY_MODEL = "gpt-4o-mini"
    MAX_SAMPLES_PER_CLUSTER = 10

# ==========================================
# 2. ë¬¸ì œë³„ ë§Œì  ì„¤ì • (extract_answers.pyì—ì„œ ê°€ì ¸ì˜´)
# ==========================================
PROBLEM_MAX_SCORES = {
    1: 40,  # 1ë²ˆ ë¬¸ì œ: 40ì  ë§Œì 
    2: 30,  # 2ë²ˆ ë¬¸ì œ: 30ì  ë§Œì 
    3: 30   # 3ë²ˆ ë¬¸ì œ: 30ì  ë§Œì 
}

# ==========================================
# 3. Helper Functions
# ==========================================
def extract_student_code_from_score(score_result: dict) -> str:
    """ì ìˆ˜í‘œ íŒŒì‹± ê²°ê³¼ì—ì„œ í•™ë²ˆ ì¶”ì¶œ"""
    return score_result.get("student_code", "unknown")

def calculate_score_distribution(score_results: list, question_num: int, max_score: int) -> tuple:
    """
    ì ìˆ˜ ë¶„í¬ ê³„ì‚°
    Returns: (scoreLabels, scoreData, avgScore)
    """
    scores = []
    for score_result in score_results:
        for answer in score_result.get("answers", []):
            if answer.get("question_number") == question_num:
                scores.append(answer.get("score", 0))
                break
    
    if not scores:
        return (["0ì ", "1-3ì ", "4-6ì ", "7-9ì ", "ë§Œì "], [0, 0, 0, 0, 0], 0.0)
    
    # ì ìˆ˜ êµ¬ê°„ë³„ ë¶„ë¥˜
    score_labels = ["0ì ", "1-3ì ", "4-6ì ", "7-9ì ", f"{max_score}ì "]
    score_data = [0, 0, 0, 0, 0]
    
    for score in scores:
        if score == 0:
            score_data[0] += 1
        elif score < 4:
            score_data[1] += 1
        elif score < 7:
            score_data[2] += 1
        elif score < max_score:
            score_data[3] += 1
        else:  # ë§Œì 
            score_data[4] += 1
    
    avg_score = sum(scores) / len(scores) if scores else 0.0
    
    return (score_labels, score_data, round(avg_score, 1))

def extract_non_perfect_answers(score_results: list, student_answers: list, problem_num: int, max_score: int) -> list:
    """
    ë§Œì ì´ ì•„ë‹Œ í•™ìƒë“¤ì˜ ë‹µì•ˆ ì¶”ì¶œ (extract_answers.py ë¡œì§)
    Returns: problem{n}_answers.json í˜•ì‹ì˜ ë°ì´í„°
    """
    non_perfect_students = []
    
    # ë§Œì ì´ ì•„ë‹Œ í•™ìƒ ì°¾ê¸°
    for score_result in score_results:
        student_code = score_result.get("student_code")
        for answer in score_result.get("answers", []):
            if answer.get("question_number") == problem_num:
                score = answer.get("score", 0)
                if score != max_score:
                    non_perfect_students.append({
                        "student_code": student_code,
                        "score": score
                    })
                break
    
    # í•´ë‹¹ í•™ìƒë“¤ì˜ ë‹µì•ˆ ì¶”ì¶œ
    extracted = []
    non_perfect_codes = {s["student_code"] for s in non_perfect_students}
    
    for answer_data in student_answers:
        student_code = answer_data.get("student_code") or answer_data.get("exam_id", "")
        if student_code in non_perfect_codes:
            # parse2.py ê²°ê³¼ í˜•ì‹: {"student_code": ..., "answers": [...]}
            # clustering.pyê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹: {"exam_id": ..., "problems": [{"problem_number": ..., "subparts": [...]}]}
            # ë³€í™˜ í•„ìš”
            
            # í•´ë‹¹ ë¬¸ì œì˜ ë‹µì•ˆ ì°¾ê¸°
            answer_item = None
            for ans in answer_data.get("answers", []):
                if ans.get("question_number") == problem_num:
                    answer_item = ans
                    break
            
            if answer_item:
                score = next((s["score"] for s in non_perfect_students if s["student_code"] == student_code), 0)
                
                # clustering.pyê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                # answer_itemì„ problem í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                problem_answer = {
                    "problem_number": problem_num,
                    "subparts": [
                        {
                            "label": "a",
                            "is_blank": False,
                            "contents": [
                                {
                                    "index": 0,
                                    "type": "text",
                                    "value": answer_item.get("answer_text", "")
                                }
                            ]
                        }
                    ]
                }
                
                extracted.append({
                    "student_code": student_code,
                    "score": score,
                    f"problem_{problem_num}_answer": problem_answer
                })
    
    return extracted

# ==========================================
# 4. Clustering ì—°ë™
# ==========================================
def run_clustering_for_problem(problem_num: int, rubric_path: str, question_image_path: str, non_perfect_answers: list):
    """
    clustering.pyë¥¼ ì‚¬ìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ìˆ˜í–‰
    """
    if not CLUSTERING_AVAILABLE or not non_perfect_answers:
        return []
    
    try:
        # problem{n}_answers.json í˜•ì‹ìœ¼ë¡œ ì„ì‹œ íŒŒì¼ ìƒì„±
        import tempfile
        temp_dir = Path(tempfile.gettempdir())
        json_path = temp_dir / f"problem{problem_num}_answers.json"
        
        # extract_answers.py í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì €ì¥
        output_data = {
            "total_count": len(non_perfect_answers),
            "problem_number": problem_num,
            "max_score": PROBLEM_MAX_SCORES.get(problem_num, 10),
            "description": f"ë¬¸ì œ {problem_num}ë²ˆ ë§Œì ì´ ì•„ë‹Œ í•™ìƒë“¤ì˜ ë‹µì•ˆ",
            "answers": non_perfect_answers
        }
        
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # clustering.pyì˜ í•¨ìˆ˜ë“¤ì„ ì§ì ‘ í˜¸ì¶œ
        exam_ids, texts = load_exams(str(json_path), problem_num)
        
        print(f"    ğŸ“Š ë¬¸ì œ {problem_num}ë²ˆ: {len(exam_ids)}ëª…ì˜ í•™ìƒ ë‹µì•ˆ ë¡œë“œë¨", file=sys.stderr)
        
        if len(exam_ids) < 2:
            # í•™ìƒì´ 2ëª… ë¯¸ë§Œì´ë©´ í´ëŸ¬ìŠ¤í„°ë§ ë¶ˆê°€
            print(f"    âš ï¸ ë¬¸ì œ {problem_num}ë²ˆ: í•™ìƒì´ {len(exam_ids)}ëª…ë¿ì´ë¼ í´ëŸ¬ìŠ¤í„°ë§ ë¶ˆê°€", file=sys.stderr)
            return []
        
        # ì„ë² ë”© ìƒì„±
        print(f"    ğŸ”„ ë¬¸ì œ {problem_num}ë²ˆ: ì„ë² ë”© ìƒì„± ì¤‘...", file=sys.stderr)
        embeddings = get_embeddings(texts)
        
        # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        print(f"    ğŸ”„ ë¬¸ì œ {problem_num}ë²ˆ: ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚° ì¤‘...", file=sys.stderr)
        sim_mat = cosine_similarity_matrix(embeddings)
        
        # í´ëŸ¬ìŠ¤í„°ë§ (ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒ)
        print(f"    ğŸ”„ ë¬¸ì œ {problem_num}ë²ˆ: í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...", file=sys.stderr)
        clusters = cluster_by_threshold(exam_ids, sim_mat, SIM_THRESHOLD)
        print(f"    âœ… ë¬¸ì œ {problem_num}ë²ˆ: {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„° ìƒì„±ë¨", file=sys.stderr)
        
        if not clusters:
            print(f"    âš ï¸ ë¬¸ì œ {problem_num}ë²ˆ: í´ëŸ¬ìŠ¤í„°ê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ (ìœ ì‚¬ë„ ì„ê³„ê°’: {SIM_THRESHOLD})", file=sys.stderr)
            return []
        
        # í†µê³„ ê³„ì‚°
        stats_per_cluster = compute_cluster_stats(exam_ids, texts, clusters, sim_mat)
        
        # OpenAIë¡œ í´ëŸ¬ìŠ¤í„° ë¶„ì„
        print(f"    ğŸ¤– ë¬¸ì œ {problem_num}ë²ˆ: OpenAIë¡œ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì¤‘...", file=sys.stderr)
        cluster_summaries = describe_clusters_with_openai(
            exam_ids,
            texts,
            clusters,
            stats_per_cluster,
            question_image_path,
            rubric_path,
            problem_num=problem_num,  # ë¬¸ì œ ë²ˆí˜¸ ì „ë‹¬
            model=CLUSTER_SUMMARY_MODEL,
            max_samples_per_cluster=MAX_SAMPLES_PER_CLUSTER
        )
        
        print(f"    âœ… ë¬¸ì œ {problem_num}ë²ˆ: {len(cluster_summaries)}ê°œ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì™„ë£Œ", file=sys.stderr)
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if json_path.exists():
            json_path.unlink()
        
        return cluster_summaries
        
    except Exception as e:
        print(f"âš ï¸ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨ (ë¬¸ì œ {problem_num}): {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return []

# ==========================================
# 5. Main Analysis Function
# ==========================================
def perform_analysis(blank_path, rubric_path, score_path, student_paths):
    """
    ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    Args:
        blank_path: ë¬¸ì œ ì›ë³¸ íŒŒì¼ ê²½ë¡œ (ë¬¸ì œì§€.pdf) â†’ test_parse.pyì˜ parse_exam() ì‚¬ìš©
        rubric_path: ì±„ì  ê¸°ì¤€í‘œ ì´ë¯¸ì§€ ê²½ë¡œ â†’ clustering.pyì˜ describe_clusters_with_openai() ì‚¬ìš©
        score_path: ì ìˆ˜í‘œ ì´ë¯¸ì§€ ê²½ë¡œ â†’ score.pyì˜ parse_sheet() ì‚¬ìš©
        student_paths: í•™ìƒ ë‹µì•ˆì§€ ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ â†’ parse2.pyì˜ parse_student_answer_handwriting() ì‚¬ìš©
    """
    print("ğŸš€ ë¶„ì„ ì‹œì‘...", file=sys.stderr)
    
    # 1. ë¬¸ì œì§€ íŒŒì‹± (test_parse.py) - ë¬¸ì œ ì›ë³¸ ì²˜ë¦¬
    print("ğŸ“ [1ë‹¨ê³„] ë¬¸ì œì§€ íŒŒì‹± ì¤‘ (test_parse.py)...", file=sys.stderr)
    try:
        problem_data = parse_exam(blank_path)  # test_parse.pyì˜ parse_exam() ì‚¬ìš©
        questions = problem_data.get("problems", [])
        print(f"âœ… ë¬¸ì œ {len(questions)}ê°œ íŒŒì‹± ì™„ë£Œ", file=sys.stderr)
    except Exception as e:
        print(f"âŒ ë¬¸ì œì§€ íŒŒì‹± ì‹¤íŒ¨: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        questions = []
    
    # 2. ì ìˆ˜í‘œ íŒŒì‹± (score.py) - ì ìˆ˜í‘œ ì²˜ë¦¬ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)
    print("ğŸ“Š [2ë‹¨ê³„] ì ìˆ˜í‘œ íŒŒì‹± ì¤‘ (score.py)...", file=sys.stderr)
    score_results = []
    # score_pathëŠ” ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ìŒ (nargs='+')
    score_paths = score_path if isinstance(score_path, list) else [score_path]
    
    print(f"  ğŸ“Š ì ìˆ˜í‘œ {len(score_paths)}ê°œ ì²˜ë¦¬ ì˜ˆì •", file=sys.stderr)
    for sp in score_paths:
        try:
            result = parse_sheet(sp)  # score.pyì˜ parse_sheet() ì‚¬ìš©
            score_results.append(result)
        except Exception as e:
            print(f"âš ï¸ ì ìˆ˜í‘œ íŒŒì‹± ì‹¤íŒ¨ ({sp}): {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
    
    total_students = len(score_results)
    print(f"âœ… ì ìˆ˜í‘œ {total_students}ê°œ íŒŒì‹± ì™„ë£Œ", file=sys.stderr)
    
    # 3. í•™ìƒ ë‹µì•ˆ íŒŒì‹± (parse2.py) - í•™ìƒ ë‹µì•ˆì§€ ì²˜ë¦¬
    print("âœï¸ [3ë‹¨ê³„] í•™ìƒ ë‹µì•ˆ íŒŒì‹± ì¤‘ (parse2.py)...", file=sys.stderr)
    student_answers = []
    
    for idx, student_path in enumerate(student_paths):
        try:
            # ì ìˆ˜í‘œì—ì„œ í•™ë²ˆ ì°¾ê¸°
            student_code = None
            if idx < len(score_results):
                student_code = score_results[idx].get("student_code")
            
            answer_data = parse_student_answer_handwriting(student_path, student_code)  # parse2.py ì‚¬ìš©
            student_answers.append(answer_data)
        except Exception as e:
            print(f"âš ï¸ ë‹µì•ˆ íŒŒì‹± ì‹¤íŒ¨ ({student_path}): {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
    
    print(f"âœ… ë‹µì•ˆ {len(student_answers)}ê°œ íŒŒì‹± ì™„ë£Œ", file=sys.stderr)
    
    # 4. ë¬¸ì œë³„ ë¶„ì„ ë° í´ëŸ¬ìŠ¤í„°ë§ (clustering.py) - ì±„ì  ê¸°ì¤€í‘œ ì‚¬ìš©
    print("ğŸ“ˆ [4ë‹¨ê³„] ë¬¸ì œë³„ ë¶„ì„ ì¤‘ (clustering.py)...", file=sys.stderr)
    questions_result = []
    
    for problem in questions:
        problem_num = problem.get("problem_index")
        max_score = problem.get("score", PROBLEM_MAX_SCORES.get(problem_num, 10))
        
        # ì ìˆ˜ ë¶„í¬ ê³„ì‚°
        score_labels, score_data, avg_score = calculate_score_distribution(
            score_results, problem_num, max_score
        )
        
        # ë§Œì ì´ ì•„ë‹Œ í•™ìƒ ë‹µì•ˆ ì¶”ì¶œ
        non_perfect_answers = extract_non_perfect_answers(
            score_results, student_answers, problem_num, max_score
        )
        
        print(f"  ğŸ“Š ë¬¸ì œ {problem_num}ë²ˆ: ë§Œì ì´ ì•„ë‹Œ í•™ìƒ {len(non_perfect_answers)}ëª…", file=sys.stderr)
        
        # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ (clustering.py ì‚¬ìš©, rubric_pathì™€ blank_path ì „ë‹¬)
        print(f"  ğŸ“Š ë¬¸ì œ {problem_num}ë²ˆ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ (clustering.py, ì±„ì  ê¸°ì¤€í‘œ ì‚¬ìš©)...", file=sys.stderr)
        clusters = run_clustering_for_problem(problem_num, rubric_path, blank_path, non_perfect_answers)
        
        print(f"  ğŸ“Š ë¬¸ì œ {problem_num}ë²ˆ: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ {len(clusters) if clusters else 0}ê°œ", file=sys.stderr)
        
        # í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì œê³µ
        if not clusters:
            clusters = [
                {
                    "cluster_index": 1,
                    "cognitive_diagnosis": {
                        "misconceptions": ["ë¶„ì„ ë°ì´í„° ë¶€ì¡±"],
                        "logical_gaps": ["ë¶„ì„ ë°ì´í„° ë¶€ì¡±"],
                        "missing_keywords": ["ë¶„ì„ ë°ì´í„° ë¶€ì¡±"]
                    },
                    "pattern_characteristics": {
                        "specificity": "ë¶„ì„ ë°ì´í„° ë¶€ì¡±",
                        "approach": "ë¶„ì„ ë°ì´í„° ë¶€ì¡±",
                        "error_type": "ë¶„ì„ ë°ì´í„° ë¶€ì¡±"
                    },
                    "quantitative_metrics": {
                        "num_students": len(non_perfect_answers),
                        "percentage": round((len(non_perfect_answers) / total_students * 100) if total_students > 0 else 0, 1),
                        "relative_length": "ë¶„ì„ ë°ì´í„° ë¶€ì¡±",
                        "expected_score_level": "ë¶„ì„ ë°ì´í„° ë¶€ì¡±"
                    },
                    "overall_summary": "í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•  ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
                }
            ]
        
        # í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        question_result = {
            "qNum": problem_num,
            "maxScore": max_score,
            "qText": problem.get("raw_text", ""),
            "avgScore": avg_score,
            "scoreLabels": score_labels,
            "scoreData": score_data,
            "clusters": clusters
        }
        
        questions_result.append(question_result)
    
    # ìµœì¢… ê²°ê³¼ ë°˜í™˜
    return {
        "totalStudents": total_students,
        "questions": questions_result
    }

# ==========================================
# 6. Main Orchestrator
# ==========================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--blank", required=True, help="ë¬¸ì œ ì›ë³¸ PDF/IMG ê²½ë¡œ")
    parser.add_argument("--rubric", required=True, help="ì±„ì  ê¸°ì¤€í‘œ IMG ê²½ë¡œ")
    parser.add_argument("--score", nargs='+', required=True, help="ì ìˆ˜í‘œ IMG ê²½ë¡œ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)")
    parser.add_argument("--students", nargs='+', required=True, help="í•™ìƒ ë‹µì•ˆ IMG ê²½ë¡œ ë¦¬ìŠ¤íŠ¸")
    args = parser.parse_args()

    try:
        # ë¶„ì„ ì‹¤í–‰ (ì—¬ëŸ¬ ì ìˆ˜í‘œ ì²˜ë¦¬)
        final_data = perform_analysis(args.blank, args.rubric, args.score, args.students)
        
        # ê²°ê³¼ë¥¼ JSON ë¬¸ìì—´ë¡œ ì¶œë ¥ (Node.jsê°€ ì½ëŠ” ë¶€ë¶„)
        print(json.dumps(final_data, ensure_ascii=False))
        
    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ stderrë¡œ ì¶œë ¥
        import traceback
        sys.stderr.write(f"Error in analysis_wrapper.py: {str(e)}\n")
        sys.stderr.write(traceback.format_exc())
        sys.exit(1)

